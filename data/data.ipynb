{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "wget \"https://ftp.ensembl.org/pub/release-113/fasta/mus_musculus/dna/Mus_musculus.GRCm39.dna.primary_assembly.fa.gz\"\n",
    "wget \"https://ftp.ensembl.org/pub/release-113/gff3/mus_musculus/Mus_musculus.GRCm39.113.gff3.gz\"\n",
    "gzip -d *.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct reference transcriptome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gffread -w GRCm39_transcripts.fa -g Mus_musculus.GRCm39.dna.primary_assembly.fa Mus_musculus.GRCm39.113.gff3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch additional annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations.py\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_fasta = sys.argv[1]\n",
    "annotations_csv = sys.argv[2]\n",
    "num_cores = int(sys.argv[3])\n",
    "batch_size = int(sys.argv[4])\n",
    "\n",
    "def fetch_chunk(chunk):\n",
    "    response = requests.post(\n",
    "        \"https://rest.ensembl.org/lookup/id\",\n",
    "        json={\"ids\": chunk},\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return list(response.json().values())\n",
    "    else:\n",
    "        raise requests.HTTPError(f'HTTP request failed with status code {response.status_code}: {response.text}')\n",
    "\n",
    "def get_transcript_info(ids):\n",
    "    args = [ids[i:(i + batch_size)] for i in range(0, len(ids), batch_size)]\n",
    "    with mp.Pool(processes=num_cores) as pool:\n",
    "        all_data = list(tqdm(pool.imap_unordered(fetch_chunk, args), total=len(args), desc=\"Fetch Data\"))\n",
    "    return [item for sublist in all_data for item in sublist if item is not None]\n",
    "\n",
    "with open(input_fasta, \"r\") as f:\n",
    "    ids = re.findall(r\"(?<=transcript:)(\\w+)(?=\\s|$)\", f.read())\n",
    "\n",
    "annotations_data = get_transcript_info(ids)\n",
    "annotations = pd.DataFrame(annotations_data)\n",
    "annotations.to_csv(annotations_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python annotations.py GRCm39_transcripts.fa annotations.csv 16 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the longest transcript\n",
    "- The in-house reference transcriptome contains the longest transcript of each gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcriptome.py\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import sys\n",
    "import subprocess as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_fasta = sys.argv[1]\n",
    "output_fasta = sys.argv[2]\n",
    "annotations_csv = sys.argv[3]\n",
    "\n",
    "def count_transcripts(input_fasta):\n",
    "    return int(sp.check_output(f\"grep -c '^>' {input_fasta}\", shell=True).decode(\"utf-8\"))\n",
    "\n",
    "annotations = pd.read_csv(annotations_csv)\n",
    "longest_transcripts = annotations.loc[annotations.groupby(\"Parent\")[\"length\"].idxmax()]\n",
    "total_transcripts = count_transcripts(input_fasta)\n",
    "\n",
    "with open(output_fasta, \"w\") as output:\n",
    "    for record in tqdm(SeqIO.parse(input_fasta, \"fasta\"), total=total_transcripts, desc=\"Filter Transcripts\"):\n",
    "        transcript_id = re.findall(r\"(?<=transcript:)(\\w+)(?=\\s|$)\", record.id)[0]\n",
    "        if transcript_id in longest_transcripts[\"id\"].values:\n",
    "            description = longest_transcripts.loc[longest_transcripts['id'] == transcript_id, 'display_name'].fillna('').iloc[0]\n",
    "            record.id = transcript_id\n",
    "            record.description = description\n",
    "            SeqIO.write(record, output, \"fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python transcriptome.py GRCm39_transcripts.fa GRCm39_lt.fa annotations.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "mkdir index\n",
    "bowtie2-build --threads 16 GRCm39_lt.fa index/GRCm39_lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Label File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python project.py sra_result.csv sra_result_projected.csv '[\"Experiment Accession\", \"Experiment Title\", \"Instrument\"]'\n",
    "python project.py SraRunInfo.csv SraRunInfo_projected.csv '[\"Run\", \"Experiment\", \"LibraryLayout\", \"SampleName\"]'\n",
    "python left-join.py sra_result_projected.csv SraRunInfo_projected.csv \"Experiment Accession\" \"Experiment\" \"Run\" SRRList_joined.csv\n",
    "python label.py SRRList_joined.csv SRRList.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import utils\n",
    "\n",
    "label_csv = sys.argv[1]\n",
    "max_size = sys.argv[2]\n",
    "num_cores = int(sys.argv[3])\n",
    "batch_size = int(sys.argv[4])\n",
    "\n",
    "def download_srr(srr):\n",
    "    executer.run([\"prefetch\", \"--max-size\", max_size, srr, \"--output-directory\", \"raw\"], f\"{srr} downloaded\")\n",
    "\n",
    "def dump_fastq(srr):\n",
    "    executer.run([\"fasterq-dump\", \"--force\", \"--verbose\", \"--split-files\", \"--outdir\", \"fastq/\", \"--temp\", \"cache/\", \"--threads\", str(num_cores), f\"raw/{srr}/{srr}.sra\"], f\"{srr} dumped\")\n",
    "\n",
    "def compress_fastq(srr):\n",
    "    if labels.are_equal(srr, {\"Layout\": [\"PAIRED\"]}):\n",
    "        executer.run([\"pigz\", \"--processes\", str(num_cores), f\"fastq/{srr}_1.fastq\", f\"fastq/{srr}_2.fastq\"], f\"{srr} compressed\")\n",
    "    else:\n",
    "        executer.run([\"pigz\", \"--processes\", str(num_cores), f\"fastq/{srr}.fastq\"], f\"{srr} compressed\")\n",
    "\n",
    "def fetch_srr(srr):\n",
    "    download_srr(srr)\n",
    "    dump_fastq(srr)\n",
    "    compress_fastq(srr)\n",
    "\n",
    "labels = utils.Label(label_csv)\n",
    "srr_list = labels.get_srr_list()\n",
    "\n",
    "executer = utils.Executer()\n",
    "executer.log(f\"SRRs to download: {srr_list}\")\n",
    "executer.run([\"mkdir\", \"-p\", \"raw\", \"fastq\"], \"directories created\")\n",
    "\n",
    "with mp.Pool(processes=batch_size) as pool, tqdm(total=len(srr_list), desc=\"Fetch Data\") as pbar:\n",
    "    for _ in pool.imap_unordered(fetch_srr, srr_list):\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python fetch.py SRRList.csv 100G 4 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process RNA-Seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnaseq.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import sam\n",
    "import utils\n",
    "\n",
    "input_dir = sys.argv[1]\n",
    "label_csv = sys.argv[2]\n",
    "bowtie2_index = sys.argv[3]\n",
    "reference_fasta = sys.argv[4]\n",
    "num_cores = int(sys.argv[5])\n",
    "batch_size = int(sys.argv[6])\n",
    "\n",
    "def decompress_fastq(srr):\n",
    "    executer.run([\"cp\", f\"{input_dir}/{srr}_1.fastq.gz\", f\"{input_dir}/{srr}_2.fastq.gz\", f\"cache/{srr}/\"], f\"{srr} copied\")\n",
    "    executer.run([\"pigz\", \"--decompress\", \"--processes\", str(num_cores), f\"cache/{srr}/{srr}_1.fastq.gz\", f\"cache/{srr}/{srr}_2.fastq.gz\"], f\"{srr} decompressed\")\n",
    "\n",
    "def trim_adaptor(srr):\n",
    "    executer.run([\"trim_galore\", \"--trim-n\", \"--output_dir\", f\"cache/{srr}/\", \"--basename\", srr, \"--cores\", str(num_cores), \"--paired\", f\"cache/{srr}/{srr}_1.fastq\", f\"cache/{srr}/{srr}_2.fastq\"], f\"{srr} trimmed\")\n",
    "\n",
    "def align_reads(srr):\n",
    "    executer.run([\"bowtie2\", \"--xeq\", \"--non-deterministic\", \"--end-to-end\", \"--very-sensitive\", \"--threads\", str(num_cores), \"-x\", bowtie2_index, \"-1\", f\"cache/{srr}/{srr}_val_1.fq\", \"-2\", f\"cache/{srr}/{srr}_val_2.fq\", \"-S\", f\"cache/{srr}/{srr}.sam\"], f\"{srr} aligned\")\n",
    "\n",
    "def process_sam(srr):\n",
    "    metrics_data = sam.count_metrics(f\"cache/{srr}/{srr}.sam\")\n",
    "    return metrics_data\n",
    "\n",
    "def process_srr(srr):\n",
    "    executer.run([\"mkdir\", \"-p\", f\"cache/{srr}/\"], f\"cache/{srr}/ created\")\n",
    "\n",
    "    decompress_fastq(srr)\n",
    "    trim_adaptor(srr)\n",
    "    align_reads(srr)\n",
    "\n",
    "    metrics_data = process_sam(srr)\n",
    "    return srr, metrics_data\n",
    "\n",
    "def write_database(srr, output_data):\n",
    "    base_name = labels.get_base_name(srr)\n",
    "    metrics.connect()\n",
    "    for ref_name, data in output_data.items():\n",
    "        key = f\"{base_name}|{ref_name}\"\n",
    "        metrics.write(key, data)\n",
    "    metrics.close()\n",
    "\n",
    "    executer.log(f\"{srr} processed\")\n",
    "    executer.run([\"rm\", \"-r\", f\"cache/{srr}\"], f\"cache/{srr}/ removed\")\n",
    "\n",
    "labels = utils.Label(label_csv)\n",
    "srr_list = labels.get_srr_list({\"Experiment\": [\"RNA-Seq\"]})\n",
    "\n",
    "metrics = utils.Database(\"metrics.db\", \"metrics\")\n",
    "executer = utils.Executer()\n",
    "executer.log(f\"SRRs to process: {srr_list}\")\n",
    "\n",
    "with mp.Pool(processes=batch_size) as pool:\n",
    "    for srr, output_data in tqdm(pool.imap_unordered(process_srr, srr_list), total=len(srr_list), desc=\"Process SRRs\"):\n",
    "        write_database(srr, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python rnaseq.py fastq SRRList.csv genome/index/GRCm39_lt genome/GRCm39_lt.fa 4 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process icSHAPE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# icshape.py\n",
    "\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import sam\n",
    "import utils\n",
    "\n",
    "input_dir = sys.argv[1]\n",
    "label_csv = sys.argv[2]\n",
    "trim_head = sys.argv[3] # 15\n",
    "bowtie2_index = sys.argv[4]\n",
    "reference_fasta = sys.argv[5]\n",
    "num_cores = int(sys.argv[6])\n",
    "batch_size = int(sys.argv[7])\n",
    "\n",
    "def decompress_fastq(srr):\n",
    "    executer.run([\"cp\", f\"{input_dir}/{srr}.fastq.gz\", f\"cache/{srr}/\"], f\"{srr} copied\")\n",
    "    executer.run([\"pigz\", \"--decompress\", \"--processes\", str(num_cores), f\"cache/{srr}/{srr}.fastq.gz\"], f\"{srr} decompressed\")\n",
    "\n",
    "def trim_adaptor(srr):\n",
    "    executer.run([\"trim_galore\", \"--trim-n\", \"--output_dir\", f\"cache/{srr}/\", \"--basename\", srr, \"--cores\", str(num_cores), f\"cache/{srr}/{srr}.fastq\"], f\"{srr} trimmed\")\n",
    "\n",
    "def collapse_reads(srr):\n",
    "    executer.run([\"clumpify.sh\", f\"in=cache/{srr}/{srr}_trimmed.fq\", f\"out=cache/{srr}/{srr}_deduped.fq\", f\"threads={str(num_cores)}\", \"dedupe=t\", \"subs=0\", \"usetmpdir=t\", \"tmpdir=cache/\"], f\"{srr} collapsed\")\n",
    "\n",
    "def remove_index(srr):\n",
    "    executer.run([\"cutadapt\", \"--cut\", trim_head, \"--cores\", str(num_cores), \"--output\", f\"cache/{srr}/{srr}_unindex.fq\", f\"cache/{srr}/{srr}_deduped.fq\"], f\"{srr} index removed\")\n",
    "\n",
    "def align_reads(srr):\n",
    "    executer.run([\"bowtie2\", \"--xeq\", \"--non-deterministic\", \"--end-to-end\", \"--very-sensitive\", \"--threads\", str(num_cores), \"-x\", bowtie2_index, \"-U\", f\"cache/{srr}/{srr}_unindex.fq\", \"-S\", f\"cache/{srr}/{srr}.sam\"], f\"{srr} aligned\")\n",
    "\n",
    "def process_sam(srr):\n",
    "    rtstops_data = sam.count_rtstops(f\"cache/{srr}/{srr}.sam\")\n",
    "    return rtstops_data\n",
    "\n",
    "def process_srr(srr):\n",
    "    executer.run([\"mkdir\", \"-p\", f\"cache/{srr}/\"], f\"cache/{srr}/ created\")\n",
    "\n",
    "    decompress_fastq(srr)\n",
    "    trim_adaptor(srr)\n",
    "    collapse_reads(srr)\n",
    "    remove_index(srr)\n",
    "    align_reads(srr)\n",
    "\n",
    "    rtstops_data = process_sam(srr)\n",
    "    return srr, rtstops_data\n",
    "\n",
    "def write_database(srr, output_data):\n",
    "    base_name = labels.get_base_name(srr)\n",
    "    rtstops.connect()\n",
    "    for ref_name, data in output_data.items():\n",
    "        key = f\"{base_name}|{ref_name}\"\n",
    "        rtstops.write(key, data)\n",
    "    rtstops.close()\n",
    "\n",
    "    executer.log(f\"{srr} processed\")\n",
    "    executer.run([\"rm\", \"-r\", f\"cache/{srr}\"], f\"cache/{srr}/ removed\")\n",
    "\n",
    "labels = utils.Label(label_csv)\n",
    "srr_list = labels.get_srr_list({\"Experiment\": [\"icSHAPE\"]})\n",
    "\n",
    "rtstops = utils.Database(\"rtstops.db\", \"rtstops\")\n",
    "executer = utils.Executer()\n",
    "executer.log(f\"SRRs to process: {srr_list}\")\n",
    "\n",
    "with mp.Pool(processes=batch_size) as pool:\n",
    "    for srr, output_data in tqdm(pool.imap_unordered(process_srr, srr_list), total=len(srr_list), desc=\"Process SRRs\"):\n",
    "        write_database(srr, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python icshape.py fastq SRRList.csv 13 genome/index/GRCm39_lt genome/GRCm39_lt.fa 4 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicates.py\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "database_db = sys.argv[1]\n",
    "table_name = sys.argv[2]\n",
    "annotations_csv = sys.argv[3]\n",
    "output_dir = sys.argv[4]\n",
    "batch_size = int(sys.argv[5])\n",
    "\n",
    "def arrange_keys(keys):\n",
    "    key_maps = dict()\n",
    "    srr_maps = dict()\n",
    "    for key in keys:\n",
    "        sample, experiment, group, srr, ref_name = key.split(\"|\")\n",
    "        legend = f\"{sample}|{experiment}|{group}\"\n",
    "        if srr not in key_maps:\n",
    "            key_maps[srr] = dict()\n",
    "        if legend not in srr_maps:\n",
    "            srr_maps[legend] = list()\n",
    "        if srr not in srr_maps[legend]:\n",
    "            srr_maps[legend].append(srr)\n",
    "        key_maps[srr][ref_name] = key\n",
    "    \n",
    "    return key_maps, srr_maps\n",
    "\n",
    "def load_data(args):\n",
    "    srr, ref_names = args\n",
    "    cache = dict()\n",
    "    data = {ref_name: 0 for ref_name in annotations.index}\n",
    "    total_reads = 0\n",
    "    for ref_name, key in ref_names.items():\n",
    "        entry_data = database.read(key)\n",
    "        entry_reads = np.sum(entry_data[\"ED\"])\n",
    "        entry_length = annotations.loc[ref_name, \"length\"]\n",
    "        total_reads += entry_reads\n",
    "        cache[ref_name] = entry_reads / entry_length\n",
    "    for ref_name, entry_data in cache.items():\n",
    "        data[ref_name] = entry_data / total_reads\n",
    "    return srr, data\n",
    "\n",
    "def group_points(data, srr_maps):\n",
    "    data_grouped = dict()\n",
    "    for legend, srr_list in srr_maps.items():\n",
    "        data_grouped[legend] = data.loc[srr_list]\n",
    "    return data_grouped\n",
    "\n",
    "# Load data\n",
    "annotations = pd.read_csv(annotations_csv)\n",
    "annotations = annotations.loc[annotations.groupby(\"Parent\")[\"length\"].idxmax()].set_index(\"id\").sort_index()\n",
    "database = utils.Database(database_db, table_name)\n",
    "database.connect()\n",
    "keys = database.list()\n",
    "key_maps, srr_maps = arrange_keys(keys)\n",
    "data = dict()\n",
    "\n",
    "with mp.Pool(processes=batch_size) as pool:\n",
    "    for srr, srr_data in tqdm(pool.imap_unordered(load_data, key_maps.items()), total=len(key_maps), desc=\"Load Data\"):\n",
    "        data[srr] = srr_data\n",
    "\n",
    "database.close()\n",
    "data = pd.DataFrame(data).transpose()\n",
    "\n",
    "# PCA decomposition\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "data_pca = pd.DataFrame(data_pca, index=data.index, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "# Create scatter plot\n",
    "data_grouped = group_points(data_pca, srr_maps)\n",
    "plt.figure(figsize=(10, 10), dpi=300)\n",
    "for legend, data in data_grouped.items():\n",
    "    plt.scatter(data.iloc[:,0], data.iloc[:,1], label=legend)\n",
    "for index in data_pca.index:\n",
    "    plt.annotate(index, (data_pca.loc[index, 'Component 1'], data_pca.loc[index, 'Component 2']))\n",
    "plt.xlabel(\"Component 1\")\n",
    "plt.ylabel(\"Component 2\")\n",
    "plt.legend()\n",
    "plt.title(table_name)\n",
    "plt.savefig(f\"{output_dir}/{table_name}_replicates.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python replicates.py metrics.db metrics genome/annotations.csv replicates 16\n",
    "python replicates.py rtstops.db rtstops genome/annotations.csv replicates 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format.py\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import sqlite3\n",
    "import utils\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import orjson\n",
    "import ast\n",
    "\n",
    "label_csv = sys.argv[1]\n",
    "metrics_db = sys.argv[2]\n",
    "rtstops_db = sys.argv[3]\n",
    "reference_fasta = sys.argv[4]\n",
    "alpha = float(sys.argv[5]) # 0.25\n",
    "strip = ast.literal_eval(sys.argv[6])\n",
    "dataset_db = sys.argv[7]\n",
    "table_name = sys.argv[8]\n",
    "batch_size = int(sys.argv[9])\n",
    "\n",
    "def onehot_encode(sequence):\n",
    "    tokens = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'U': [0, 0, 0, 1], 'N': [0.25, 0.25, 0.25, 0.25]}\n",
    "    encoded = [tokens[nt] for nt in sequence]\n",
    "    encoded = np.transpose(encoded).tolist()\n",
    "    return encoded\n",
    "\n",
    "def winsorize_scale(scores):\n",
    "    keys = list(scores.keys())\n",
    "    values = list(scores.values())\n",
    "\n",
    "    lower_threshold = np.percentile(values, 5)\n",
    "    upper_threshold = np.percentile(values, 95)\n",
    "\n",
    "    if lower_threshold == upper_threshold:\n",
    "        lower_threshold = np.min(values)\n",
    "        upper_threshold = np.max(values)\n",
    "        winsorized_values = values\n",
    "    else:\n",
    "        winsorized_values = np.clip(values, lower_threshold, upper_threshold)\n",
    "\n",
    "    if lower_threshold == upper_threshold != 0:\n",
    "        scaled_values = winsorized_values / upper_threshold\n",
    "    elif lower_threshold == upper_threshold == 0:\n",
    "        scaled_values = winsorized_values\n",
    "    else:\n",
    "        scaled_values = (winsorized_values - lower_threshold) / (upper_threshold - lower_threshold)\n",
    "\n",
    "    results = {key: float(value) for key, value in zip(keys, scaled_values)}\n",
    "    return results\n",
    "\n",
    "def filter_entries(keys):\n",
    "    ref_names = dict()\n",
    "    sample_list = labels.unique_values(\"Sample\")\n",
    "    for sample in sample_list:\n",
    "        ref_names[sample] = dict()\n",
    "        srr_list = labels.get_srr_list({\"Sample\": [sample]})\n",
    "        for srr in srr_list:\n",
    "            ref_names[sample][srr] = list()\n",
    "\n",
    "    for key in keys:\n",
    "        sample = key.split(\"|\")[0]\n",
    "        srr = key.split(\"|\")[3]\n",
    "        ref_name = key.split(\"|\")[4]\n",
    "        ref_names[sample][srr].append(ref_name)\n",
    "\n",
    "    results = dict()\n",
    "    for sample in ref_names:\n",
    "        results[sample] = list(set.intersection(*[set(ref_names[sample][srr]) for srr in ref_names[sample]]))\n",
    "\n",
    "    return results\n",
    "\n",
    "def read_fasta(filename):\n",
    "    sequences = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        current_seq_id = None\n",
    "        sequence_data = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()  # Remove leading/trailing whitespace\n",
    "            if line.startswith('>'):\n",
    "                if current_seq_id:  # Store previous sequence\n",
    "                    sequences[current_seq_id] = sequence_data\n",
    "                current_seq_id = line[1:].split(' ')[0]  # Extract ID from header\n",
    "                sequence_data = \"\"\n",
    "            else:\n",
    "                sequence_data += line\n",
    "        if current_seq_id:  # Store last sequence\n",
    "            sequences[current_seq_id] = sequence_data\n",
    "    return sequences\n",
    "\n",
    "def load_reactivity(sample, ref_name):\n",
    "    NAIN3_list = labels.get_srr_list({\"Sample\": [sample], \"Experiment\": [\"icSHAPE\"], \"Group\": [\"NAIN3\"]})\n",
    "    DMSO_list = labels.get_srr_list({\"Sample\": [sample], \"Experiment\": [\"icSHAPE\"], \"Group\": [\"DMSO\"]})\n",
    "\n",
    "    DMSO_depth = dict()\n",
    "    DMSO_stop = dict()\n",
    "    NAIN3_stop = dict()\n",
    "\n",
    "    num_rep = len(DMSO_list)\n",
    "    for srr in DMSO_list:\n",
    "        key = f\"{sample}|icSHAPE|DMSO|{srr}|{ref_name}\"\n",
    "        rtstops_entry = rtstops.read(key)\n",
    "        for index, pos in enumerate(rtstops_entry[\"PS\"]):\n",
    "            if pos not in DMSO_depth:\n",
    "                DMSO_depth[pos] = 0\n",
    "            if pos not in DMSO_stop:\n",
    "                DMSO_stop[pos] = 0\n",
    "            DMSO_depth[pos] += rtstops_entry[\"RD\"][index] / num_rep\n",
    "            DMSO_stop[pos] += rtstops_entry[\"ED\"][index] / num_rep\n",
    "\n",
    "    num_rep = len(NAIN3_list)\n",
    "    for srr in NAIN3_list:\n",
    "        key = f\"{sample}|icSHAPE|NAIN3|{srr}|{ref_name}\"\n",
    "        rtstops_entry = rtstops.read(key)\n",
    "        for index, pos in enumerate(rtstops_entry[\"PS\"]):\n",
    "            if pos not in NAIN3_stop:\n",
    "                NAIN3_stop[pos] = 0\n",
    "            NAIN3_stop[pos] += rtstops_entry[\"ED\"][index] / num_rep\n",
    "\n",
    "    start = min(min(DMSO_stop.keys()), min(NAIN3_stop.keys())) # 1-based position\n",
    "    end = max(max(DMSO_stop.keys()), max(NAIN3_stop.keys())) # 1-based position\n",
    "        \n",
    "    md_indicators = dict() # missing data indicators, 0b001: DMSO missing, 0b010: NAIN3 missing, 0b100: RNA-Seq missing\n",
    "    reactivity_scores = dict()\n",
    "    for pos in range(start, end + 1):\n",
    "        md_indicators[pos] = 0\n",
    "        reactivity_scores[pos] = 0\n",
    "        if pos not in DMSO_stop:\n",
    "            md_indicators[pos] |= 0b001\n",
    "        if pos not in NAIN3_stop:\n",
    "            md_indicators[pos] |= 0b010\n",
    "        if md_indicators[pos] == 0:\n",
    "            reactivity_scores[pos] = (NAIN3_stop[pos] - alpha * DMSO_stop[pos]) / DMSO_depth[pos]\n",
    "\n",
    "    total_density = np.sum(list(DMSO_depth.values()))\n",
    "    reactivity_scores = winsorize_scale(reactivity_scores)\n",
    "\n",
    "    return reactivity_scores, md_indicators, total_density\n",
    "\n",
    "def load_metrics(sample, ref_name):\n",
    "    srr_list = labels.get_srr_list({\"Sample\": [sample], \"Experiment\": [\"RNA-Seq\"]})\n",
    "    \n",
    "    keys = list()\n",
    "    for srr in srr_list:\n",
    "        key = f\"{sample}|RNA-Seq|NA|{srr}|{ref_name}\"\n",
    "        if key in metrics_keys:\n",
    "            keys.append(key)\n",
    "    \n",
    "    num_rep = len(keys)\n",
    "    if num_rep == 0:\n",
    "        raise ValueError(\"no RNA-Seq data\")\n",
    "    \n",
    "    read_depth = dict()\n",
    "    end_depth = dict()\n",
    "    end_rate = dict()\n",
    "    mismatch_count = dict()\n",
    "    mismatch_rate = dict()\n",
    "\n",
    "    for key in keys:\n",
    "        metrics_entry = metrics.read(key)\n",
    "        for index, pos in enumerate(metrics_entry[\"PS\"]):\n",
    "            if pos not in read_depth:\n",
    "                read_depth[pos] = 0\n",
    "            if pos not in end_depth:\n",
    "                end_depth[pos] = 0\n",
    "            if pos not in mismatch_count:\n",
    "                mismatch_count[pos] = 0\n",
    "            read_depth[pos] += metrics_entry[\"RD\"][index] / num_rep\n",
    "            end_depth[pos] += metrics_entry[\"ED\"][index] / num_rep\n",
    "            mismatch_count[pos] += metrics_entry[\"MC\"][index] / num_rep\n",
    "\n",
    "    for pos in read_depth:\n",
    "        end_rate[pos] = end_depth[pos] / read_depth[pos]\n",
    "        mismatch_rate[pos] = mismatch_count[pos] / read_depth[pos]\n",
    "\n",
    "    total_depth = np.sum(list(read_depth.values()))\n",
    "    total_end_rate = np.sum(list(end_rate.values()))\n",
    "    total_mismatch_rate = np.sum(list(mismatch_rate.values()))\n",
    "    read_depth = winsorize_scale(read_depth)\n",
    "\n",
    "    return read_depth, end_rate, mismatch_rate, total_depth, total_end_rate, total_mismatch_rate\n",
    "\n",
    "def format_entry(args):\n",
    "    sample, ref_name = args\n",
    "    entry_name = f\"{sample}|{ref_name}\"\n",
    "\n",
    "    try:\n",
    "        reactivity_scores_dict, reactivity_indicators_dict, total_density = load_reactivity(sample, ref_name)\n",
    "        read_depth_dict, end_rate_dict, mismatch_rate_dict, total_depth, total_end_rate, total_mismatch_rate = load_metrics(sample, ref_name)\n",
    "    except ValueError as e:\n",
    "        executer.log(f\"{e} for {entry_name}\")\n",
    "        return None\n",
    "    \n",
    "    sequence = reference_transcriptome[ref_name].replace(\"T\", \"U\")\n",
    "    full_length = len(sequence)\n",
    "    channel_A, channel_C, channel_G, channel_U = onehot_encode(sequence)\n",
    "    \n",
    "    indicators = [0] * full_length # 0b001: DMSO missing, 0b010: NAIN3 missing, 0b100: RNA-Seq missing\n",
    "    reactivity = [0] * full_length\n",
    "    read_depth = [0] * full_length\n",
    "    end_rate = [0] * full_length\n",
    "    mismatch_rate = [0] * full_length\n",
    "    for pos in range(1, full_length + 1):\n",
    "        if pos in reactivity_scores_dict:\n",
    "            reactivity[pos - 1] = reactivity_scores_dict[pos]\n",
    "            indicators[pos - 1] |= reactivity_indicators_dict[pos]\n",
    "        else:\n",
    "            indicators[pos - 1] |= 0b011\n",
    "        if pos in read_depth_dict:\n",
    "            read_depth[pos - 1] = read_depth_dict[pos]\n",
    "            end_rate[pos - 1] = end_rate_dict[pos]\n",
    "            mismatch_rate[pos - 1] = mismatch_rate_dict[pos]\n",
    "        else:\n",
    "            indicators[pos - 1] |= 0b100\n",
    "\n",
    "    # find the first and last pos where indicator == 0\n",
    "    start = None # 1-based position\n",
    "    end = None # (-1)-based position in reverse\n",
    "    for index in range(full_length):\n",
    "        if indicators[index] == 0 and start is None:\n",
    "            start = index + 1\n",
    "        if indicators[index] == 0:\n",
    "            end = index - full_length\n",
    "            \n",
    "    if (start is None) or (end is None) or (start - 1 >= end + full_length):\n",
    "        executer.log(f\"no valid data for {entry_name}\")\n",
    "        return None\n",
    "    \n",
    "    valid_length = end + full_length - start + 2\n",
    "    strip_length = full_length - valid_length\n",
    "    gap = 0\n",
    "    for index in range(start - 1, end + full_length + 1):\n",
    "        if indicators[index] != 0:\n",
    "            gap += 1\n",
    "    mean_depth = total_depth / valid_length\n",
    "    mean_end = total_end_rate / valid_length\n",
    "    mean_density = total_density / valid_length\n",
    "    mean_mismatch = total_mismatch_rate / valid_length\n",
    "\n",
    "    if strip:\n",
    "        sequence = sequence[(start - 1):(end + full_length + 1)]\n",
    "        channel_A = channel_A[(start - 1):(end + full_length + 1)]\n",
    "        channel_C = channel_C[(start - 1):(end + full_length + 1)]\n",
    "        channel_G = channel_G[(start - 1):(end + full_length + 1)]\n",
    "        channel_U = channel_U[(start - 1):(end + full_length + 1)]\n",
    "        read_depth = read_depth[(start - 1):(end + full_length + 1)]\n",
    "        end_rate = end_rate[(start - 1):(end + full_length + 1)]\n",
    "        mismatch_rate = mismatch_rate[(start - 1):(end + full_length + 1)]\n",
    "        reactivity = reactivity[(start - 1):(end + full_length + 1)]\n",
    "        indicators = indicators[(start - 1):(end + full_length + 1)]\n",
    "\n",
    "    entry = (entry_name, \n",
    "             orjson.dumps(channel_A), orjson.dumps(channel_C), orjson.dumps(channel_G), orjson.dumps(channel_U), orjson.dumps(read_depth), orjson.dumps(end_rate), orjson.dumps(mismatch_rate), orjson.dumps(reactivity), orjson.dumps(indicators), \n",
    "             sample, ref_name, sequence, start, end, full_length, valid_length, strip_length,\n",
    "             mean_depth, mean_end, mean_density, mean_mismatch, gap\n",
    "            )\n",
    "    return entry\n",
    "\n",
    "labels = utils.Label(label_csv)\n",
    "reference_transcriptome = read_fasta(reference_fasta)\n",
    "\n",
    "metrics = utils.Database(metrics_db, \"metrics\")\n",
    "rtstops = utils.Database(rtstops_db, \"rtstops\")\n",
    "executer = utils.Executer()\n",
    "\n",
    "metrics.connect()\n",
    "rtstops.connect()\n",
    "metrics_keys = metrics.list()\n",
    "rtstops_keys = rtstops.list()\n",
    "ref_names = filter_entries(rtstops_keys + metrics_keys)\n",
    "args = [(sample, ref_name) for sample in ref_names for ref_name in ref_names[sample]]\n",
    "\n",
    "dataset = sqlite3.connect(dataset_db)\n",
    "dataset_cursor = dataset.cursor()\n",
    "dataset_cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} (SeqID TEXT PRIMARY KEY, A TEXT, C TEXT, G TEXT, U TEXT, RD TEXT, ER TEXT, MR TEXT, RT TEXT, IC TEXT, Sample TEXT, RefName TEXT, Sequence TEXT, Start INT, End INT, FullLength INT, ValidLength INT, StripLength INT, MeanDepth REAL, MeanEnd REAL, MeanDensity REAL, MeanMismatch REAL, Gap INT)\")\n",
    "\n",
    "with mp.Pool(processes=batch_size) as pool:\n",
    "    for entry in tqdm(pool.imap_unordered(format_entry, args), total=len(args), desc=\"Format Dataset\"):\n",
    "        if entry is not None:\n",
    "            dataset_cursor.execute(f\"INSERT INTO {table_name} VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", entry)\n",
    "\n",
    "dataset.commit()\n",
    "dataset.close()\n",
    "metrics.close()\n",
    "rtstops.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python format.py SRRList.csv metrics.db rtstops.db genome/GRCm39_lt.fa 0.25 False mouse.db mouse 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# references.py\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "annotations_csv = sys.argv[1]\n",
    "dataset_db = sys.argv[2]\n",
    "\n",
    "annotations = pd.read_csv(annotations_csv)\n",
    "annotations = annotations.loc[annotations.groupby(\"Parent\")[\"length\"].idxmax()].sort_values(by=\"id\")\n",
    "dataset = sqlite3.connect(dataset_db)\n",
    "cursor = dataset.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS ref (RefName TEXT PRIMARY KEY, DisplayName TEXT, Biotype TEXT, Start INT, End INT, Length INT, Strand INT, Parent TEXT, SeqRegionName INT, Canonical INT, GENCODEPrimary INT, AssemblyName TEXT, Version INT)\")\n",
    "\n",
    "for _, row in annotations.iterrows():\n",
    "    display_name = row[\"display_name\"]\n",
    "    if display_name is np.nan:\n",
    "        display_name = row[\"id\"]\n",
    "    entry = (row[\"id\"], display_name, row[\"biotype\"], row[\"start\"], row[\"end\"], row[\"length\"], row[\"strand\"], row[\"Parent\"], row[\"seq_region_name\"], row[\"is_canonical\"], row[\"gencode_primary\"], row[\"assembly_name\"], row[\"version\"])\n",
    "    cursor.execute(f\"INSERT INTO ref VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\", entry)\n",
    "\n",
    "dataset.commit()\n",
    "dataset.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python references.py genome/annotations.csv mouse.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembly.py\n",
    "\n",
    "import sqlite3\n",
    "import re\n",
    "import sys\n",
    "\n",
    "source_db = sys.argv[1]\n",
    "source_table = sys.argv[2]\n",
    "target_db = sys.argv[3]\n",
    "target_table = sys.argv[4]\n",
    "\n",
    "source_conn = sqlite3.connect(source_db)\n",
    "source_cursor = source_conn.cursor()\n",
    "target_conn = sqlite3.connect(target_db)\n",
    "target_cursor = target_conn.cursor()\n",
    "\n",
    "source_cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{source_table}'\")\n",
    "source_schema = source_cursor.fetchone()\n",
    "source_schema = re.search(r'\\((.*)\\)', source_schema, re.DOTALL).group(1).strip()\n",
    "\n",
    "target_cursor.execute(f\"CREATE TABLE IF NOT EXISTS {target_table} ({source_schema})\")\n",
    "target_cursor.execute(f\"ATTACH DATABASE '{source_db}' AS source\")\n",
    "target_cursor.execute(f\"PRAGMA table_info('{target_table}');\")\n",
    "target_pragma = target_cursor.fetchall()\n",
    "target_columns = [col[1] for col in target_pragma]\n",
    "\n",
    "insert_columns = \", \".join(target_columns)\n",
    "select_columns = \", \".join([f\"source.{source_table}.{col}\" for col in target_columns])\n",
    "\n",
    "query = f\"INSERT INTO {target_table} ({insert_columns}) SELECT {select_columns} FROM source.{source_table}\"\n",
    "print(query)\n",
    "target_cursor.execute(query)\n",
    "target_conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python assembly.py /home/test/xwt/pred/data/neural/process/neural.db neural assembly.db assembly\n",
    "python assembly.py /home/test/xwt/pred/data/zebrafish/process/zebrafish.db zebrafish assembly.db assembly\n",
    "python assembly.py /home/test/xwt/pred/data/neural/process/neural.db ref assembly.db ref\n",
    "python assembly.py /home/test/xwt/pred/data/zebrafish/process/zebrafish.db ref assembly.db ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram.py\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "database_db = sys.argv[1]\n",
    "table_name = sys.argv[2]\n",
    "features = ast.literal_eval(sys.argv[3])\n",
    "clause = sys.argv[4]\n",
    "num_bins = int(sys.argv[5])\n",
    "logarithmic = ast.literal_eval(sys.argv[6])\n",
    "eps = float(sys.argv[7])\n",
    "output_dir = sys.argv[8]\n",
    "\n",
    "conn = sqlite3.connect(database_db)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for feature in tqdm(features, desc=\"Calculate Features\"):\n",
    "    cursor.execute(f\"SELECT {feature} FROM {table_name} {clause}\")\n",
    "    if logarithmic:\n",
    "        data = [np.log10(abs(float(row[0])) + eps) for row in cursor.fetchall()]\n",
    "    else:\n",
    "        data = [float(row[0]) for row in cursor.fetchall()]\n",
    "    plt.figure(figsize=(10, 5), dpi=300)\n",
    "    plt.hist(data, bins=num_bins, edgecolor=\"white\")\n",
    "    if logarithmic:\n",
    "        plt.xlabel(f\"$\\\\log_{{10}}$(|{feature}| + {eps})\")\n",
    "    else:\n",
    "        plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"{table_name}: {feature}\")\n",
    "    plt.savefig(f\"{output_dir}/{table_name}_{feature}.png\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python histogram.py ../assembly.db assembly \"['MeanEnd', 'MeanMismatch']\" \"\" 100 True 1e-5 original\n",
    "python histogram.py ../assembly.db assembly \"['Start', 'End', 'FullLength', 'ValidLength', 'StripLength', 'MeanDepth', 'MeanDensity', 'Gap']\" \"\" 100 True  1 original\n",
    "python histogram.py ../assembly.db assembly \"['MeanEnd', 'MeanMismatch']\" \"WHERE FullLength BETWEEN 64 AND 4096 AND StripLength <= 16 AND MeanDepth >= 16 AND MeanDensity >= 64 AND Gap = 0\" 100 True 1e-5 filtered\n",
    "python histogram.py ../assembly.db assembly \"['Start', 'End', 'FullLength', 'ValidLength', 'StripLength', 'MeanDepth', 'MeanDensity', 'Gap']\" \"WHERE FullLength BETWEEN 64 AND 4096 AND StripLength <= 16 AND MeanDepth >= 16 AND MeanDensity >= 64 AND Gap = 0\" 100 True 1 filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informtiation.py\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import sqlite3\n",
    "import sys\n",
    "import orjson\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "database_db = sys.argv[1]\n",
    "table_name = sys.argv[2]\n",
    "key_col = sys.argv[3]\n",
    "features = ast.literal_eval(sys.argv[4])\n",
    "clause = sys.argv[5]\n",
    "output_dir = sys.argv[6]\n",
    "\n",
    "conn = sqlite3.connect(database_db)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"SELECT {', '.join(features)} FROM {table_name} {clause}\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "data = {feature: list() for feature in features}\n",
    "\n",
    "for row in tqdm(rows, desc=\"Load Data\"):\n",
    "    for feature, value in zip(features, row):\n",
    "        data[feature].extend(orjson.loads(value))\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "column_names = list(data.columns)\n",
    "data = data.to_numpy()\n",
    "n_features = data.shape[1]\n",
    "mi_matrix = list()\n",
    "\n",
    "for i in tqdm(range(n_features), desc=\"Compute MI\"):\n",
    "    mi_matrix.append(mutual_info_regression(data, data[:, i]))\n",
    "\n",
    "mi_matrix = pd.DataFrame(mi_matrix, columns=column_names, index=column_names)\n",
    "mi_matrix.to_csv(f'{output_dir}/information.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python information.py ../assembly.db assembly SeqID \"['A', 'C', 'G', 'U', 'RD', 'ER', 'MR', 'IC', 'RT']\" \"WHERE FullLength BETWEEN 64 AND 4096 AND StripLength <= 16 AND MeanDepth >= 16 AND MeanDensity >= 64 AND Gap = 0\" filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motif completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completeness.py\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "database_db = sys.argv[1]\n",
    "table_name = sys.argv[2]\n",
    "clause = sys.argv[3]\n",
    "max_len = int(sys.argv[4])\n",
    "output_dir = sys.argv[5]\n",
    "batch_size = int(sys.argv[6])\n",
    "\n",
    "def add_dict(this, other):\n",
    "    for key, val in other.items():\n",
    "        if key in this:\n",
    "            this[key] += val\n",
    "        else:\n",
    "            this[key] = val\n",
    "    return this\n",
    "\n",
    "def count_motif(sequence, motif_length):\n",
    "    end = len(sequence) - motif_length + 1\n",
    "    motifs = dict()\n",
    "    for start in range(motif_length):\n",
    "        for i in range(start, end, motif_length):\n",
    "            motif = sequence[i:i+motif_length]\n",
    "            if 'N' in motif:\n",
    "                continue\n",
    "            if motif in motifs:\n",
    "                motifs[motif] += 1\n",
    "            else:\n",
    "                motifs[motif] = 1\n",
    "    return motifs\n",
    "\n",
    "def summarize_counts(motif_counts):\n",
    "    completeness = dict()\n",
    "    p_values = dict()\n",
    "    for motif_length in range(1, max_len + 1):\n",
    "        observed = len(list(motif_counts[motif_length].values()))\n",
    "        expected = 4**motif_length\n",
    "        observed_counts = np.array(list(motif_counts[motif_length].values()))\n",
    "        expected_counts = observed_counts.sum() / expected\n",
    "        completeness[motif_length] = observed / expected\n",
    "        chi_squared = np.sum((observed_counts - expected_counts)**2 / expected_counts) + (expected - observed) * expected_counts\n",
    "        p_values[motif_length] = stats.chi2.sf(chi_squared, expected - 1)\n",
    "    return completeness, p_values\n",
    "\n",
    "conn = sqlite3.connect(database_db)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"SELECT Sequence FROM {table_name} {clause}\")\n",
    "rows = cursor.fetchall()\n",
    "sequences = [row[0] for row in rows]\n",
    "motif_counts = dict()\n",
    "conn.close()\n",
    "\n",
    "with mp.Pool(processes=batch_size) as pool:\n",
    "    for motif_length in tqdm(range(1, max_len + 1), desc=f\"Count Motif\"):\n",
    "        partial_count = partial(count_motif, motif_length=motif_length)\n",
    "        motif_counts[motif_length] = dict()\n",
    "        for motifs in pool.imap_unordered(partial_count, sequences):\n",
    "            motif_counts[motif_length] = add_dict(motif_counts[motif_length], motifs)\n",
    "\n",
    "completeness, p_values = summarize_counts(motif_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "plt.plot(list(completeness.keys()), list(completeness.values()))\n",
    "plt.xlabel(\"Motif Length\")\n",
    "plt.ylabel(\"Proportion Complete\")\n",
    "plt.title(f\"{table_name}: Completeness\")\n",
    "plt.savefig(f\"{output_dir}/{table_name}_completeness.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 5), dpi=300)\n",
    "plt.plot(list(p_values.keys()), list(p_values.values()))\n",
    "plt.xlabel(\"Motif Length\")\n",
    "plt.ylabel(\"P-Value Balanced\")\n",
    "plt.title(f\"{table_name}: Balance\")\n",
    "plt.savefig(f\"{output_dir}/{table_name}_balance.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python completeness.py ../assembly.db assembly \"WHERE FullLength BETWEEN 64 AND 4096 AND StripLength <= 16 AND MeanDepth >= 16 AND MeanDensity >= 64 AND Gap = 0\" 16 filtered 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.py\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "database_db = sys.argv[1]\n",
    "output_csv = sys.argv[2]\n",
    "query = sys.argv[3]\n",
    "\n",
    "conn = sqlite3.connect(database_db)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "col_names = [description[0] for description in cursor.description]\n",
    "conn.close()\n",
    "\n",
    "df = dict()\n",
    "for col_name in col_names:\n",
    "    df[col_name] = list()\n",
    "for row in rows:\n",
    "    for col_name, value in zip(col_names, row):\n",
    "        if type(value) == bytes:\n",
    "            value = value.decode()\n",
    "        df[col_name].append(value)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python query.py assembly.db sample.csv \"SELECT * FROM assembly WHERE FullLength BETWEEN 64 AND 4096 AND StripLength <= 16 AND MeanDepth >= 16 AND MeanDensity >= 64 AND Gap = 0 ORDER BY RANDOM() LIMIT 10\"\n",
    "python query.py assembly.db assembly.csv \"SELECT * FROM assembly WHERE FullLength BETWEEN 64 AND 4096 AND StripLength <= 16 AND MeanDepth >= 16 AND MeanDensity >= 64 AND Gap = 0 ORDER BY RANDOM()\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update.py\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "database_db = sys.argv[1]\n",
    "dataset_csv = sys.argv[2]\n",
    "key_column = sys.argv[3]\n",
    "query = sys.argv[4]\n",
    "\n",
    "conn = sqlite3.connect(database_db)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "col_names = [description[0] for description in cursor.description]\n",
    "conn.close()\n",
    "\n",
    "df = dict()\n",
    "for col_name in col_names:\n",
    "    df[col_name] = list()\n",
    "for row in rows:\n",
    "    for col_name, value in zip(col_names, row):\n",
    "        if type(value) == bytes:\n",
    "            value = value.decode()\n",
    "        df[col_name].append(value)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "data = pd.read_csv(dataset_csv)\n",
    "data = pd.merge(data, df, on=key_column, how=\"outer\")\n",
    "data.to_csv(dataset_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python update.py assembly.db assembly.csv SeqID \"SELECT SeqID, RibonanzaNetPredictions, RibonanzaNetMAE FROM assembly WHERE FullLength BETWEEN 64 AND 4096 AND StripLength <= 16 AND MeanDepth >= 16 AND MeanDensity >= 64 AND Gap = 0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import orjson\n",
    "import ast\n",
    "import subprocess\n",
    "import datetime\n",
    "\n",
    "class Label:\n",
    "    def __init__(self, label_csv):\n",
    "        self.data = pd.read_csv(label_csv, keep_default_na=False, na_values=[])\n",
    "        self.sanity_check()\n",
    "        self.data[\"SRR\"] = self.data[\"SRR\"].apply(ast.literal_eval)\n",
    "\n",
    "    def sanity_check(self):\n",
    "        for _, row in self.data.iterrows():\n",
    "            if row[\"Layout\"] != \"SINGLE\" and row[\"Layout\"] != \"PAIRED\":\n",
    "                raise ValueError(f\"invalid library layout type: {row[\"Layout\"]}\")\n",
    "            if (row[\"Experiment\"] == \"RNA-Seq\" and row[\"Layout\"] != \"PAIRED\") or (row[\"Experiment\"] == \"icSHAPE\" and row[\"Layout\"] != \"SINGLE\"):\n",
    "                raise ValueError(f\"incorrect library layout for {row[\"GSM\"]}\")\n",
    "            if (row[\"Experiment\"] == \"RNA-Seq\" and row[\"Group\"] != \"NA\") or (row[\"Experiment\"] == \"icSHAPE\" and (row[\"Group\"] != \"DMSO\" and row[\"Group\"] != \"NAIN3\")):\n",
    "                raise ValueError(f\"incorrect experiment group for {row[\"GSM\"]}\")\n",
    "            \n",
    "    def get_row(self, srr):\n",
    "        rows = self.data[self.data[\"SRR\"].apply(lambda x: srr in x)]\n",
    "        if rows.shape[0] > 1:\n",
    "            raise ValueError(f\"{srr} found in multiple rows\")\n",
    "        elif rows.shape[0] == 0:\n",
    "            raise ValueError(f\"{srr} not found\")\n",
    "        row = rows.iloc[0]\n",
    "        return row\n",
    "    \n",
    "    def unique_values(self, property):\n",
    "        values = self.data[property].unique().tolist()\n",
    "        return values\n",
    "\n",
    "    def get_base_name(self, srr):\n",
    "        row = self.get_row(srr)\n",
    "        base_name = f\"{row[\"Sample\"]}|{row[\"Experiment\"]}|{row[\"Group\"]}|{srr}\"\n",
    "        return base_name\n",
    "    \n",
    "    def are_equal(self, srr, map):\n",
    "        row = self.get_row(srr)\n",
    "        indicators = list()\n",
    "        for property, value in map.items():\n",
    "            indicators.append(row[property] in value)\n",
    "        return all(indicators)\n",
    "    \n",
    "    def get_srr_list(self, map=dict()):\n",
    "        if len(map) == 0:\n",
    "            srr_lists = self.data[\"SRR\"].tolist()\n",
    "            srr_list = [srr for item in srr_lists for srr in item]\n",
    "        else:\n",
    "            filtered_data = self.data\n",
    "            for property, value in map.items():\n",
    "                filtered_data = filtered_data[filtered_data[property].isin(value)]\n",
    "            srr_lists = filtered_data[\"SRR\"].tolist()\n",
    "            srr_list = [srr for item in srr_lists for srr in item]\n",
    "        return srr_list\n",
    "\n",
    "class Database:\n",
    "    def __init__(self, database_db, table_name):\n",
    "        self.database_db = database_db\n",
    "        self.table_name = table_name\n",
    "\n",
    "    def connect(self):\n",
    "        self.conn = sqlite3.connect(self.database_db)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute(f\"CREATE TABLE IF NOT EXISTS {self.table_name} (key TEXT PRIMARY KEY, value TEXT)\")\n",
    "\n",
    "    def write(self, key, value):\n",
    "        serialized_value = orjson.dumps(value)\n",
    "        self.cursor.execute(f\"INSERT INTO {self.table_name} VALUES (?, ?)\", (key, serialized_value))\n",
    "\n",
    "    def read(self, key):\n",
    "        self.cursor.execute(f\"SELECT value FROM {self.table_name} WHERE key = ?\", (key,))\n",
    "        value = self.cursor.fetchone()[0]\n",
    "        retrieved = orjson.loads(value)\n",
    "        return retrieved\n",
    "    \n",
    "    def list(self):\n",
    "        self.cursor.execute(f\"SELECT key FROM {self.table_name}\")\n",
    "        keys = [row[0] for row in self.cursor.fetchall()]\n",
    "        return keys\n",
    "    \n",
    "    def close(self):\n",
    "        self.conn.commit()\n",
    "        self.conn.close()\n",
    "\n",
    "class Executer:\n",
    "    def __init__(self, log_file=\"logs.txt\", executable_path=\"/bin/bash\"):\n",
    "        self.logs = open(log_file, \"a\")\n",
    "        self.executable = executable_path\n",
    "\n",
    "    def run(self, command, message):\n",
    "        self.log(str(command))\n",
    "        subprocess.run(command, stdout=self.logs, stderr=self.logs)\n",
    "        self.log(message)\n",
    "\n",
    "    def shell(self, script, message):\n",
    "        self.log(script)\n",
    "        subprocess.run(script, shell=True, executable=self.executable, stdout=self.logs, stderr=self.logs)\n",
    "        self.log(message)\n",
    "\n",
    "    def log(self, message):\n",
    "        self.logs.write(f\"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: {message}\\n\")\n",
    "        self.logs.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAM parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sam.py\n",
    "\n",
    "import pysam\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_metrics(sam_file):\n",
    "    sam_data = pysam.AlignmentFile(sam_file, \"rb\")\n",
    "    metrics_count = defaultdict(lambda: defaultdict(lambda: {\"RD\": 0, \"ED\": 0, \"MC\": 0}))\n",
    "\n",
    "    for read in sam_data.fetch():\n",
    "        flag = read.flag\n",
    "\n",
    "        if (not (flag & 3 == 3)) or (flag & (4 | 8 | 256 | 512 | 1024 | 2048)):\n",
    "            continue\n",
    "            \n",
    "        rname = sam_data.get_reference_name(read.reference_id)\n",
    "        pos = read.reference_start + 1\n",
    "        \n",
    "        # Process CIGAR string\n",
    "        indicator = []\n",
    "        for op, count in read.cigartuples:\n",
    "            if op == 2:  # D\n",
    "                indicator.extend(['D'] * count)\n",
    "            elif op == 7:  # =\n",
    "                indicator.extend(['='] * count)\n",
    "            elif op == 8:  # X\n",
    "                indicator.extend(['X'] * count)\n",
    "\n",
    "        # Count end depth\n",
    "        end = pos + len(indicator) - 1 if flag & 16 else pos\n",
    "        metrics_count[rname][end][\"ED\"] += 1\n",
    "        \n",
    "        # Batch process read depth and mismatches\n",
    "        for index, char in enumerate(indicator):\n",
    "            bp = pos + index\n",
    "            metrics_count[rname][bp][\"RD\"] += 1\n",
    "            if char != '=':\n",
    "                metrics_count[rname][bp][\"MC\"] += 1\n",
    "\n",
    "    sam_data.close()\n",
    "\n",
    "    # Convert to final format\n",
    "    metrics_data = {}\n",
    "    for rname, positions in metrics_count.items():\n",
    "        metrics_entry = {\"PS\": [], \"RD\": [], \"ED\": [], \"MC\": []}\n",
    "        for bp, counts in sorted(positions.items()):\n",
    "            metrics_entry[\"PS\"].append(bp)\n",
    "            metrics_entry[\"RD\"].append(counts[\"RD\"])\n",
    "            metrics_entry[\"ED\"].append(counts[\"ED\"])\n",
    "            metrics_entry[\"MC\"].append(counts[\"MC\"])\n",
    "        metrics_data[rname] = metrics_entry\n",
    "\n",
    "    return metrics_data\n",
    "\n",
    "def count_rtstops(sam_file):\n",
    "    sam_data = pysam.AlignmentFile(sam_file, \"rb\")\n",
    "    rtstops_count = defaultdict(lambda: defaultdict(lambda: {\"RD\": 0, \"ED\": 0, \"MC\": 0}))\n",
    "\n",
    "    for read in sam_data.fetch():\n",
    "        flag = read.flag\n",
    "\n",
    "        if flag:\n",
    "            continue\n",
    "            \n",
    "        rname = sam_data.get_reference_name(read.reference_id)\n",
    "        pos = read.reference_start + 1\n",
    "        \n",
    "        # Process CIGAR string\n",
    "        indicator = []\n",
    "        for op, count in read.cigartuples:\n",
    "            if op == 2:  # D\n",
    "                indicator.extend(['D'] * count)\n",
    "            elif op == 7:  # =\n",
    "                indicator.extend(['='] * count)\n",
    "            elif op == 8:  # X\n",
    "                indicator.extend(['X'] * count)\n",
    "\n",
    "        # Count end depth\n",
    "        rtstops_count[rname][pos][\"ED\"] += 1\n",
    "        \n",
    "        # Batch process read depth and mismatches\n",
    "        for index, char in enumerate(indicator):\n",
    "            bp = pos + index\n",
    "            rtstops_count[rname][bp][\"RD\"] += 1\n",
    "            if char != '=':\n",
    "                rtstops_count[rname][bp][\"MC\"] += 1\n",
    "\n",
    "    sam_data.close()\n",
    "\n",
    "    # Convert to final format\n",
    "    rtstops_data = {}\n",
    "    for rname, positions in rtstops_count.items():\n",
    "        rtstops_entry = {\"PS\": [], \"RD\": [], \"ED\": [], \"MC\": []}\n",
    "        for bp, counts in sorted(positions.items()):\n",
    "            rtstops_entry[\"PS\"].append(bp)\n",
    "            rtstops_entry[\"RD\"].append(counts[\"RD\"])\n",
    "            rtstops_entry[\"ED\"].append(counts[\"ED\"])\n",
    "            rtstops_entry[\"MC\"].append(counts[\"MC\"])\n",
    "        rtstops_data[rname] = rtstops_entry\n",
    "\n",
    "    return rtstops_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left join CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left-join.py\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def left_join_csv(file1, file2, left_col, right_col, multiple_map_col):\n",
    "    \"\"\"Performs a left join on two CSV files with specified column mappings.\n",
    "\n",
    "    Args:\n",
    "        file1 (str): Path to the first CSV file.\n",
    "        file2 (str): Path to the second CSV file.\n",
    "        left_col (str): Column name from file1 to use in the join.\n",
    "        right_col (str): Corresponding column name from file2 to use in the join.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting DataFrame after the join.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data from CSV files into Pandas DataFrames\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    def aggregate_columns(group):\n",
    "        \"\"\"Aggregates columns from the right table, using a list or single value.\"\"\"\n",
    "        result = {}\n",
    "        for col in df2.columns:\n",
    "            if col != right_col and col != multiple_map_col:  # Exclude the 'SRX' column used for joining\n",
    "                values = group[col].unique()\n",
    "                assert len(values) == 1\n",
    "                result[col] = values[0]\n",
    "            elif col == multiple_map_col:\n",
    "                values = group[col].unique()\n",
    "                result[col] = list(values)\n",
    "        return pd.Series(result)\n",
    "\n",
    "    # Perform the inner join \n",
    "    joined_df = pd.merge(df1, df2, left_on=left_col, right_on=right_col, how='left')\n",
    "\n",
    "    grouped_df = joined_df.groupby(left_col).apply(aggregate_columns, include_groups=False).reset_index()\n",
    "\n",
    "    final_df = pd.merge(df1, grouped_df, on=left_col, how=\"left\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file1 = sys.argv[1]\n",
    "    file2 = sys.argv[2]\n",
    "\n",
    "    # Define corresponding column names for the join\n",
    "    left_col = sys.argv[3]\n",
    "    right_col = sys.argv[4]\n",
    "    multiple_map_col = sys.argv[5]\n",
    "\n",
    "    result_df = left_join_csv(file1, file2, left_col, right_col, multiple_map_col)\n",
    "\n",
    "    result_df.to_csv(sys.argv[6], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project.py\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import ast\n",
    "\n",
    "def project_dataframe(input_file, columns_to_keep):\n",
    "    \"\"\"Projects a CSV table, returning a pandas DataFrame with specified columns.\n",
    "\n",
    "    Args:\n",
    "        input_file: The path to the input CSV file.\n",
    "        columns_to_keep: A list of column titles to keep.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing only the specified columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the CSV directly into a DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Check if all columns_to_keep exist in the DataFrame\n",
    "    valid_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "    \n",
    "    # Return the projected DataFrame\n",
    "    return df[valid_columns]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get user input (or modify this section for direct file paths)\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    columns_to_keep = ast.literal_eval(sys.argv[3])\n",
    "\n",
    "    projected_df = project_dataframe(input_file, columns_to_keep)\n",
    "    projected_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label.py\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def label_experiment(source):\n",
    "    data = {\"GSM\": [], \"SRR\": [], \"Sample\": [], \"Experiment\": [], \"Group\": [], \"Repetition\": [], \"Layout\": [], \"Instrument\": []}\n",
    "    for index, row in source.iterrows():\n",
    "        data[\"GSM\"].append(row[\"SampleName\"])\n",
    "        data[\"SRR\"].append(row[\"Run\"])\n",
    "        data[\"Layout\"].append(row[\"LibraryLayout\"])\n",
    "        data[\"Instrument\"].append(row[\"Instrument\"])\n",
    "        label = row[\"Experiment Title\"]\n",
    "        \n",
    "        # Sample\n",
    "        if \"0h\" in label:\n",
    "            data[\"Sample\"].append(\"0h\")\n",
    "        elif \"4h\" in label:\n",
    "            data[\"Sample\"].append(\"4h\")\n",
    "        else:\n",
    "            data[\"Sample\"].append(\"OTHER\")\n",
    "\n",
    "        # Experiment & Group\n",
    "        if (\"-N\" in label) and (\"icSHAPE\" in label) :\n",
    "            data[\"Experiment\"].append(\"icSHAPE\")\n",
    "            data[\"Group\"].append(\"NAIN3\")\n",
    "        elif (\"-D\" in label) and (\"icSHAPE\" in label):\n",
    "            data[\"Experiment\"].append(\"icSHAPE\")\n",
    "            data[\"Group\"].append(\"DMSO\")\n",
    "        elif \"WT\" in label:\n",
    "            data[\"Experiment\"].append(\"RNA-Seq\")\n",
    "            data[\"Group\"].append(\"NA\")\n",
    "        else:\n",
    "            data[\"Experiment\"].append(\"OTHER\")\n",
    "            data[\"Group\"].append(\"OTHER\")\n",
    "\n",
    "        # Repetition\n",
    "        if (\"-1\" in label) or (\"-N1\" in label) or (\"-D1\" in label):\n",
    "            data[\"Repetition\"].append(\"1\")\n",
    "        elif (\"-2\" in label) or (\"-N2\" in label) or (\"-D2\" in label):\n",
    "            data[\"Repetition\"].append(\"2\")\n",
    "        elif (\"-3\" in label) or (\"-N3\" in label) or (\"-D3\" in label):\n",
    "            data[\"Repetition\"].append(\"3\")\n",
    "        else:\n",
    "            data[\"Repetition\"].append(\"NA\")\n",
    "    \n",
    "    # Remove OTHER rows\n",
    "    df = pd.DataFrame(data)\n",
    "    filtered = df[~df.isin(['OTHER']).any(axis=1)]\n",
    "    return filtered\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = sys.argv[1]\n",
    "    output_file = sys.argv[2]\n",
    "    source = pd.read_csv(input_file)\n",
    "    \n",
    "    df = label_experiment(source)\n",
    "    df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge.py\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "def merge_csv(file1, file2):\n",
    "    \"\"\"Merges two CSV files with the same header.\n",
    "\n",
    "    Args:\n",
    "        file1 (str): Path to the first CSV file.\n",
    "        file2 (str): Path to the second CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The resulting DataFrame after merging.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data from CSV files into Pandas DataFrames\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Concatenate the two DataFrames\n",
    "    merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file1 = sys.argv[1]\n",
    "    file2 = sys.argv[2]\n",
    "\n",
    "    result_df = merge_csv(file1, file2)\n",
    "\n",
    "    result_df.to_csv(sys.argv[3], index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
